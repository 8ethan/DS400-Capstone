{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3310f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58764b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SCARP_Kafka_Test')\\\n",
    "                    .config('spark.jars.packages','org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1')\\\n",
    "                    .config('spark.jars.packages','org.apache.kafka:kafka-clients:2.4.1')\\\n",
    "                    .config(\"spark.driver.memory\", \"16g\")\\\n",
    "                    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c85f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"SCARP_kafka_notebook\")\\\n",
    "    .option('includeTimestamp', 'true')\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4726fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017cfe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "data_df = df.select(['value','timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2000eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowed dataframes\n",
    "\n",
    "df_window = ( \n",
    "    data_df.groupBy(window(data_df['timestamp'], \"10 seconds\", \"10 seconds\"), data_df['value'])\\\n",
    "    .count()\\\n",
    "    .orderBy('window')\\\n",
    "    #.withColumnRenamed('count(follower)','TotalAmount')\n",
    ")\n",
    "\n",
    "df_window2 = ( \n",
    "    data_df.groupBy(window(data_df['timestamp'], \"10 seconds\", \"10 seconds\"), data_df['value'])\\\n",
    "    .count()\\\n",
    "    .orderBy(desc('window'))\\\n",
    "    #.withColumnRenamed('count(follower)','TotalAmount')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd9ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowed query. Doesn't work for our purpose.\n",
    "\n",
    "query = df_window2.writeStream\\\n",
    "        .outputMode('complete')\\\n",
    "        .format('memory')\\\n",
    "        .queryName('window_test')\\\n",
    "        .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f0648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing windowed query\n",
    "\n",
    "import time\n",
    "for x in range(30):\n",
    "    _df = spark.sql('SELECT * FROM window_test')\n",
    "    _df.show(10, truncate=50)\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38162b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original atttempt at inference\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#GBTmodel = GBTClassificationModel.load(\"./models/gbt_best_model\")\n",
    "    \n",
    "prev_timestamp = []\n",
    "totalMalware = 0\n",
    "totalBenign = 0\n",
    "\n",
    "while True:\n",
    "    _df = spark.sql('SELECT * FROM projectML WHERE ... ORDER BY timestamp DESC')\n",
    "\n",
    "    if _df.first() is not None:\n",
    "\n",
    "        cur_timestamp = _df.select('timestamp').collect()\n",
    "\n",
    "        if prev_timestamp != cur_timestamp:\n",
    "            #display.clear_output(wait=True)\n",
    "            prev_timestamp = cur_timestamp\n",
    "            print(\"New Data Found...\")\n",
    "            extractor = PEFeatureExtractor(2)\n",
    "            features = np.array(extractor.feature_vector(bytes(_df.first().value)), dtype=np.float32)\n",
    "\n",
    "            prediction = GBTmodel.predict(DenseVector(features))\n",
    "            print(\"Prediction: \", prediction)\n",
    "\n",
    "            if prediction==1:\n",
    "                totalMalware += 1\n",
    "            elif prediction==0:\n",
    "                totalBenign += 1\n",
    "\n",
    "            print(\"Total Malware:\\t\",+str(totalMalware))\n",
    "            print(\"Total Benign:\\t\"+str(totalBenign))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e694a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working query\n",
    "\n",
    "query2 = data_df.writeStream\\\n",
    "        .format('memory')\\\n",
    "        .queryName('stream_inference')\\\n",
    "        .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7951ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792bf237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "import ember_modified\n",
    "from ember_modified.features import PEFeatureExtractor\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acac8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in model\n",
    "GBTmodel = GBTClassificationModel.load(\"./gbt500k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ceaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalMalware = 0\n",
    "totalBenign = 0\n",
    "\n",
    "extractor = PEFeatureExtractor(2)\n",
    "\n",
    "while (_df.count() == 0):\n",
    "    _df = spark.sql('SELECT * FROM stream_inference ORDER BY timestamp DESC')\n",
    "\n",
    "prev_timestamp = _df.select('timestamp').collect()[0].asDict()['timestamp']\n",
    "        \n",
    "print(\"Entering loop\")\n",
    "while True:\n",
    "    \n",
    "    _df = spark.sql('SELECT * FROM stream_inference WHERE timestamp > \\''+str(prev_timestamp)+'\\' ORDER BY timestamp DESC')\n",
    "    \n",
    "    if _df.count() != 0:\n",
    "        \n",
    "        #_df.show(truncate=40)\n",
    "        df_data = _df.collect()\n",
    "        \n",
    "        prev_timestamp = df_data[0].asDict()['timestamp']\n",
    "        \n",
    "        for row in df_data:\n",
    "            #if (row.asDict()['timestamp'] > prev_timestamp):\n",
    "            #    prev_timestamp = row.asDict()['timestamp']\n",
    "            #start_time = time.time_ns()\n",
    "    \n",
    "            raw_data = row.asDict()['value']\n",
    "            features = extractor.feature_vector_spark(bytes(raw_data))\n",
    "            prediction = GBTmodel.predict(features)\n",
    "            \n",
    "            if prediction==1:\n",
    "                totalMalware+=1\n",
    "            else:\n",
    "                totalBenign+=1\n",
    "            \n",
    "            #end_time = time.time_ns()\n",
    "            #print( int((end_time - start_time)/1000000) )\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        #print(\"New prev timestamp: \", prev_timestamp)\n",
    "        #_df.show(truncate=40)\n",
    "        print(\"Total Malware: \"+str(totalMalware)+\"\\tTotal Benign: \"+str(totalBenign))\n",
    "        #time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f90247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Improvements\n",
    "# Partition raw data into different features\n",
    "# Feature importance study for potentially removing non-essential features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
